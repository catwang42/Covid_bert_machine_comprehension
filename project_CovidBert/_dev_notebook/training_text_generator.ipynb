{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "from pprint import pprint\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import chain\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up file path and Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_PATH=\"../training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = '{GLOBAL_PATH}/metadata.csv'.format(GLOBAL_PATH=GLOBAL_PATH)\n",
    "\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zjufx4fo</td>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>10.1093/emboj/20.24.7220</td>\n",
       "      <td>PMC125340</td>\n",
       "      <td>11742998</td>\n",
       "      <td>unk</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>2001-12-17</td>\n",
       "      <td>Pasternak, Alexander O.; van den Born, Erwin; ...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125340?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ymceytj3</td>\n",
       "      <td>e3d0d482ebd9a8ba81c254cc433f314142e72174</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n",
       "      <td>10.1093/emboj/21.9.2076</td>\n",
       "      <td>PMC125375</td>\n",
       "      <td>11980704</td>\n",
       "      <td>unk</td>\n",
       "      <td>CEACAM1 is a member of the carcinoembryonic an...</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125375?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wzj2glte</td>\n",
       "      <td>00b1d99e70f779eb4ede50059db469c65e8c1469</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Synthesis of a novel hepatitis C virus protein...</td>\n",
       "      <td>10.1093/emboj/20.14.3840</td>\n",
       "      <td>PMC125543</td>\n",
       "      <td>11447125</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Hepatitis C virus (HCV) is an important human ...</td>\n",
       "      <td>2001-07-16</td>\n",
       "      <td>Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...</td>\n",
       "      <td>EMBO J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
       "1  ymceytj3  e3d0d482ebd9a8ba81c254cc433f314142e72174      PMC   \n",
       "2  wzj2glte  00b1d99e70f779eb4ede50059db469c65e8c1469      PMC   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sequence requirements for RNA strand transfer ...   \n",
       "1  Crystal structure of murine sCEACAM1a[1,4]: a ...   \n",
       "2  Synthesis of a novel hepatitis C virus protein...   \n",
       "\n",
       "                        doi      pmcid pubmed_id license  \\\n",
       "0  10.1093/emboj/20.24.7220  PMC125340  11742998     unk   \n",
       "1   10.1093/emboj/21.9.2076  PMC125375  11980704     unk   \n",
       "2  10.1093/emboj/20.14.3840  PMC125543  11447125   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
       "1  CEACAM1 is a member of the carcinoembryonic an...   2002-05-01   \n",
       "2  Hepatitis C virus (HCV) is an important human ...   2001-07-16   \n",
       "\n",
       "                                             authors           journal  \\\n",
       "0  Pasternak, Alexander O.; van den Born, Erwin; ...  The EMBO Journal   \n",
       "1  Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...  The EMBO Journal   \n",
       "2  Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...            EMBO J   \n",
       "\n",
       "  Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                         NaN            NaN           True   \n",
       "1                         NaN            NaN           True   \n",
       "2                         NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  http://europepmc.org/articles/pmc125340?pdf=re...  \n",
       "1  http://europepmc.org/articles/pmc125375?pdf=re...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read BioXiv paper for testing training generator functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "biorxiv = '/biorxiv_medrxiv'\n",
    "bioxiv_json = glob.glob(f'{GLOBAL_PATH+biorxiv}/**/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/f6b29be971089bfe0916c64ab9fbddcec38a7436.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bioxiv_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  2278  pdf Json in BioXiv\n",
      "===================================================\n",
      "Dictionary keys: dict_keys(['paper_id', 'metadata', 'abstract', 'body_text', 'bib_entries', 'ref_entries', 'back_matter'])\n"
     ]
    }
   ],
   "source": [
    "print(\"There are: \",len(bioxiv_json),\" pdf Json in BioXiv\")\n",
    "print(\"===================================================\")\n",
    "all_files = []\n",
    "for filename in bioxiv_json:\n",
    "    #filename = biorxiv_dir + filename\n",
    "    file = json.load(open(filename, 'rb'))\n",
    "    all_files.append(file)\n",
    "\n",
    "file = all_files[0]\n",
    "print(\"Dictionary keys:\", file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cite_spans': [],\n",
      "  'ref_spans': [],\n",
      "  'section': 'Abstract',\n",
      "  'text': 'Aims: To determine analytical capabilities of a commonly used '\n",
      "          'faecal immunochemical test (FIT) to detect haemoglobin (Hb) in the '\n",
      "          'context of NICE guidance DG30, and the likely use of FIT to '\n",
      "          'reprioritise patients delayed by the COVID-19 pandemic.'},\n",
      " {'cite_spans': [],\n",
      "  'ref_spans': [],\n",
      "  'section': 'Abstract',\n",
      "  'text': 'Methods: Data obtained from independent verification studies and '\n",
      "          'clinical testing of the HM-JACKarc FIT method in routine primary '\n",
      "          'care practice were analysed to derive analytical performance '\n",
      "          'characteristics.'},\n",
      " {'cite_spans': [],\n",
      "  'ref_spans': [],\n",
      "  'section': 'Abstract',\n",
      "  'text': 'Results: Detection capabilities for the FIT method were 0.5 µg/g '\n",
      "          '(limit of blank), 1.1 (limit of detection) and 15.0 µg/g (limit of '\n",
      "          'quantification). 31 of 33 (94%) non-homogenised specimens analysed '\n",
      "          'in triplicate were consistently categorised relative to 10 µg/g '\n",
      "          'compared to all 33 (100%) homogenised specimens. Imprecision in '\n",
      "          'non-homogenised specimens was higher (median 27.8%, (range 20.5% '\n",
      "          '-48.6%)) than in homogenised specimens (10.'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(file['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_text content:\n",
      "[{'cite_spans': [{...}, {...}, {...}],\n",
      "  'ref_spans': [],\n",
      "  'section': 'Introduction',\n",
      "  'text': 'Colorectal cancer is globally the third most incident malignancy '\n",
      "          '(1) . It is surgically treatable with improved long-term outcomes '\n",
      "          'if diagnosis is at an early stage (2) . Most developed countries, '\n",
      "          'including the UK, operate colorectal screening programmes using '\n",
      "          'faecal occult blood testing. Screen detected cancers benefit from '\n",
      "          'early diagnosis and treatment, with associated improved survival '\n",
      "          '(2) . Faecal immunochemical tests (FIT) have largely replaced the '\n",
      "          'traditional guaiac based faecal occult blood tests due to the '\n",
      "          'increased specificity of FIT.'},\n",
      " {'cite_spans': [{...}, {...}],\n",
      "  'ref_spans': [],\n",
      "  'section': 'Introduction',\n",
      "  'text': 'To complement the UK Bowel Cancer Screening Programme (UKBCSP), the '\n",
      "          '2017 DG30 NICE guidance (3) recommended the use of FIT for faecal '\n",
      "          'haemoglobin (Hb) detection in patients presenting to primary care '\n",
      "          'with low risk abdominal symptoms. The adoption of FIT in primary '\n",
      "          'care has been slow, with notable variation in uptake and '\n",
      "          'implementation across the UK (4). The Oxford University Hospitals '\n",
      "          'Trust (OUH) adopted FIT prior to the DG30 guidance to comply with '\n",
      "          'the 2015 NG12 NICE guidance for suspected cancer (6) , which '\n",
      "          'recommended the use of faecal occult blood testing in symptomatic '\n",
      "          'patients. This coincided with a desire from the clinical laboratory '\n",
      "          'to move away from the comparatively inaccurate guaiac based method. '\n",
      "          'FIT was commissioned by Oxfordshire Clinical Commissioning Group '\n",
      "          '(OCCG) as a direct access test for General Practitioners in 2016 '\n",
      "          'following a study by this group comparing the accuracy of the '\n",
      "          'guaiac and FIT methods in symptomatic primary care patients meeting '\n",
      "          'NG12 criteria (5) .'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"body_text content:\")\n",
    "pprint(file['body_text'][:2], depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction',\n",
      " 'Setting and analytical method',\n",
      " 'Method detection capability estimates and immunoassay reproducibility',\n",
      " 'Effects of sample homogenisation',\n",
      " 'Stability of specimens within the collection device',\n",
      " 'Within patient serial sampling',\n",
      " 'Statistical analysis',\n",
      " 'Detection capabilities and immunoassay reproducibility',\n",
      " 'Discussion',\n",
      " 'Implications for research and practice.',\n",
      " 'Concluding remarks.']\n"
     ]
    }
   ],
   "source": [
    "texts = [(di['section'], di['text']) for di in file['body_text']]\n",
    "texts_di = {di['section']: \"\" for di in file['body_text']}\n",
    "for section, text in texts:\n",
    "    texts_di[section] += text\n",
    "\n",
    "pprint(list(texts_di.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "\n",
      "Colorectal cancer is globally the third most incident malignancy (1) . It is surgically treatable with improved long-term outcomes if diagnosis is at an early stage (2) . Most developed countries, including the UK, operate colorectal screening programmes using faecal occult blood testing. Screen detected cancers benefit from early diagnosis and treatment, with associated improved survival (2) . Faecal immunochemical tests (FIT) have largely replaced the traditional guaiac based faecal occult blood tests due to the increased specificity of FIT.To complement the UK Bowel Cancer Screening Programme (UKBCSP), the 2017 DG30 NICE guidance (3) recommended the use of FIT for faecal haemoglobin (Hb) detection in patients presenting to primary care with low risk abdominal symptoms. The adoption of FIT in primary care has been slow, with notable variation in uptake and implementation across the UK (4). The Oxford University Hospitals Trust (OUH) adopted FIT prior to the DG30 guidance to comply with the 2015 NG12 NICE guidance for suspected cancer (6) , which recommended the use of faecal occult blood testing in symptomatic patients. This coincided with a desire from the clinical laboratory to move away from the comparatively inaccurate guaiac based method. FIT was commissioned by Oxfordshire Clinical Commissioning Group (OCCG) as a direct access test for General Practitioners in 2016 following a study by this group comparing the accuracy of the guaiac and FIT methods in symptomatic primary care patients meeting NG12 criteria (5) .Despite gradual uptake of FIT, there remains a clear need to understand FIT method characteristics (7) . FIT testing, whether undertaken within a screening programme or applied to a symptomatic population, is dependent on the analytical performance of the laboratory procedures used. These analytical characteristics, most crucially sampling, impact on the way the results are most accurately reported (8) . Whilst there has been significant work undertaken in the context of screening (9) there is less data available in the symptomatic population. This is important to explore as the characteristics of the population for a screening programme, and the associated specimens, may differ from patients presenting with symptoms. For example, the age-range of the symptomatic population is broader and specimen characteristics, including faecal consistency, will likely differ in patients with changes in bowel habit, and the information and support for taking a sample is less. This may affect sample, and sampling integrity.The FIT methods used to process samples from symptomatic patients are those developed for measuring faecal Hb at higher concentrations in screening. At present, the cut-off used by the UKBCSP is >120 µg/g, with future plans to reduce this to approximately 50 µg/g to improve sensitivity. The threshold recommended for use with symptomatic patients in DG30 is 10 µg/g. FIT methods may not be optimised, or fully charac\n"
     ]
    }
   ],
   "source": [
    "body = \"\"\n",
    "\n",
    "for section, text in texts_di.items():\n",
    "    body += section\n",
    "    body += \"\\n\\n\"\n",
    "    body += text\n",
    "    body += \"\\n\\n\"\n",
    "\n",
    "print(body[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flatten the Json file to create the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "\n",
    "#first_row = FileReader(bioxiv_json[0])\n",
    "#print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0 of 2278\n",
      "Processing index: 227 of 2278\n",
      "Processing index: 454 of 2278\n",
      "Processing index: 681 of 2278\n",
      "Processing index: 908 of 2278\n",
      "Processing index: 1135 of 2278\n",
      "Processing index: 1362 of 2278\n",
      "Processing index: 1589 of 2278\n",
      "Processing index: 1816 of 2278\n",
      "Processing index: 2043 of 2278\n",
      "Processing index: 2270 of 2278\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f6b29be971089bfe0916c64ab9fbddcec38a7436</td>\n",
       "      <td>Aims: To determine analytical capabilities of ...</td>\n",
       "      <td>Colorectal cancer is globally the third most i...</td>\n",
       "      <td>37232    Faecal immunochemical testing (FIT): ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005d189d5bd7ac01aee65e934fd3d5186a3f7b27</td>\n",
       "      <td>The rapid outbreak of the new Coronavirus pand...</td>\n",
       "      <td>The outbreak of infectious diseases has always...</td>\n",
       "      <td>36952    Relationship between Average Daily Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f905f78b32f63c6d14a79984dfb33f1b358b8ab4</td>\n",
       "      <td>New anti-AIDS treatments must be continually d...</td>\n",
       "      <td>In the absence of a curative treatment, the hi...</td>\n",
       "      <td>37478    Multimerization of HIV-1 integrase hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>607e0074d8ff40c272b958c2fe48793fedfc785e</td>\n",
       "      <td></td>\n",
       "      <td>the author/funder, who has granted medRxiv a l...</td>\n",
       "      <td>36290    Virus shedding patterns in nasopharyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72e25b728c6c62fb3a4e2c59c8ee48de4b5ee452</td>\n",
       "      <td>Recently classified as a pandemic by WHO, nove...</td>\n",
       "      <td>The pandemic of COVID-19 is taking a troll in ...</td>\n",
       "      <td>37135    Phylogenetic Analysis of the Novel Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  f6b29be971089bfe0916c64ab9fbddcec38a7436   \n",
       "1  005d189d5bd7ac01aee65e934fd3d5186a3f7b27   \n",
       "2  f905f78b32f63c6d14a79984dfb33f1b358b8ab4   \n",
       "3  607e0074d8ff40c272b958c2fe48793fedfc785e   \n",
       "4  72e25b728c6c62fb3a4e2c59c8ee48de4b5ee452   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Aims: To determine analytical capabilities of ...   \n",
       "1  The rapid outbreak of the new Coronavirus pand...   \n",
       "2  New anti-AIDS treatments must be continually d...   \n",
       "3                                                      \n",
       "4  Recently classified as a pandemic by WHO, nove...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Colorectal cancer is globally the third most i...   \n",
       "1  The outbreak of infectious diseases has always...   \n",
       "2  In the absence of a curative treatment, the hi...   \n",
       "3  the author/funder, who has granted medRxiv a l...   \n",
       "4  The pandemic of COVID-19 is taking a troll in ...   \n",
       "\n",
       "                                               title  \n",
       "0  37232    Faecal immunochemical testing (FIT): ...  \n",
       "1  36952    Relationship between Average Daily Te...  \n",
       "2  37478    Multimerization of HIV-1 integrase hi...  \n",
       "3  36290    Virus shedding patterns in nasopharyn...  \n",
       "4  37135    Phylogenetic Analysis of the Novel Co...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'title': []}\n",
    "for idx, entry in enumerate(bioxiv_json):\n",
    "    if idx % (len(bioxiv_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(bioxiv_json)}')\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    dict_['title'].append(meta_data['title'])\n",
    "\n",
    "    \n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'title'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2278"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_covid[\"body_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # lowercase text\n",
    "    text = str(text).lower()\n",
    "    # remove non-UTF\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "    # remove punktuation symbols\n",
    "    text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def sentencise_text(text):\n",
    "    text = sent_tokenize(text)\n",
    "    return text \n",
    "\n",
    "def count_lines(filename):\n",
    "    count = 0\n",
    "    with open(filename) as fi:\n",
    "        for line in fi:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize_text(df_covid[\"body_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRC_DATA_FPATH = \"../data/pretrain/proc_dataset.txt\" #@param {type: \"string\"}\n",
    "with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in df_covid[\"body_text\"]:\n",
    "        fo.write(normalize_text(l)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving sentence per line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-90-e4c15c597b3b>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-e4c15c597b3b>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    fo.write(sentencise_text(normalize_text(l))+\"\\n\")\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PRC_DATA_FPATH = \"../data/pretrain/proc_sentence_dataset.txt\" #@param {type: \"string\"}\n",
    "sentencised =[]\n",
    "\n",
    "with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in df_covid[\"body_text\"][:10]:\n",
    "        sentencised.append(sentencise_text(normalize_text(I))\n",
    "\n",
    "        fo.write(sentencise_text(normalize_text(l))+\"\\n\")\n",
    "        fo.writelines(\"%s\\n\" % place for place in places_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run all dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_json = glob.glob('{GLOBAL_PATH}/**/*.json'.format(GLOBAL_PATH=GLOBAL_PATH), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0 of  {68204}\n",
      "Processing index: 6820 of  {68204}\n",
      "Processing index: 13640 of  {68204}\n",
      "Processing index: 20460 of  {68204}\n",
      "Processing index: 27280 of  {68204}\n",
      "Processing index: 34100 of  {68204}\n",
      "Processing index: 40920 of  {68204}\n",
      "Processing index: 47740 of  {68204}\n",
      "Processing index: 54560 of  {68204}\n",
      "Processing index: 61380 of  {68204}\n",
      "Processing index: 68200 of  {68204}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2cd9b37d110968368ce6f837e002788ac6158af8</td>\n",
       "      <td>Background: Tuberculosis is a leading cause of...</td>\n",
       "      <td>Tuberculosis (TB) is one of the top ten causes...</td>\n",
       "      <td>12096    Under-reporting of TB cases and assoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298d325e27d8cafcd150d56a199a5ce099ee2b07</td>\n",
       "      <td>The emergence of severe acute respiratory synd...</td>\n",
       "      <td>The Chinese National Influenza Center of the C...</td>\n",
       "      <td>24962    A ten-year China-US laboratory collab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6e2b81ea145f20bad9129013f611181c0fe4ad8a</td>\n",
       "      <td>Newcastle disease virus (NDV) infection causes...</td>\n",
       "      <td>Newcastle disease (ND) is a highly contagious ...</td>\n",
       "      <td>16626    Newcastle disease virus RNA-induced I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aefd921eef67855fd84f460502e9e7277aeb92a1</td>\n",
       "      <td>Globalization has been accompanied by the rapi...</td>\n",
       "      <td>Working conditions for health workers are unde...</td>\n",
       "      <td>2071    Collaboration between infection contro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7b0510f8258fc50bcff6f0e1a66c54c04093f9d1</td>\n",
       "      <td></td>\n",
       "      <td>With a 2000-year medicinal history, Radix Bupl...</td>\n",
       "      <td>9870    A systematic review of the active saik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  2cd9b37d110968368ce6f837e002788ac6158af8   \n",
       "1  298d325e27d8cafcd150d56a199a5ce099ee2b07   \n",
       "2  6e2b81ea145f20bad9129013f611181c0fe4ad8a   \n",
       "3  aefd921eef67855fd84f460502e9e7277aeb92a1   \n",
       "4  7b0510f8258fc50bcff6f0e1a66c54c04093f9d1   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Background: Tuberculosis is a leading cause of...   \n",
       "1  The emergence of severe acute respiratory synd...   \n",
       "2  Newcastle disease virus (NDV) infection causes...   \n",
       "3  Globalization has been accompanied by the rapi...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Tuberculosis (TB) is one of the top ten causes...   \n",
       "1  The Chinese National Influenza Center of the C...   \n",
       "2  Newcastle disease (ND) is a highly contagious ...   \n",
       "3  Working conditions for health workers are unde...   \n",
       "4  With a 2000-year medicinal history, Radix Bupl...   \n",
       "\n",
       "                                               title  \n",
       "0  12096    Under-reporting of TB cases and assoc...  \n",
       "1  24962    A ten-year China-US laboratory collab...  \n",
       "2  16626    Newcastle disease virus RNA-induced I...  \n",
       "3  2071    Collaboration between infection contro...  \n",
       "4  9870    A systematic review of the active saik...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'title': []}\n",
    "for idx, entry in enumerate(load_json):\n",
    "    if idx % (len(load_json) // 10) == 0:\n",
    "        print('Processing index: {idx} of '.format(idx=idx),{len(load_json)})\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    dict_['title'].append(meta_data['title'])\n",
    "\n",
    "    \n",
    "    \n",
    "df_covid_all = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'title'])\n",
    "df_covid_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41117, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Remove duplicates and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid_all.drop_duplicates(['title'], inplace=True)\n",
    "#df_covid_all.dropna(subset=['body_text'], inplace=True)\n",
    "df_covid_new = df_covid_all[~df_covid_all['title'].apply(tuple).duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40694, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply preprocessig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)\n",
    "    text = re.sub(r'(\\w\\s){3,}', ' ', text)\n",
    "    # replace tags (e.g. [NUM1]) with whitespace\n",
    "    text = re.sub(r'\\[[\\d\\,\\s]+\\]\\s', ' ',text)\n",
    "    # correctly spacing the tokens\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    # return lowercase text\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py365/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_covid_new['body_text'] = df_covid_new['body_text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Search for covid related paper only (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n",
    "              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n",
    "              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\n",
    "covid_terms = [preprocessing(elem) for elem in covid_terms]\n",
    "covid_terms = re.compile('|'.join(covid_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'covid|coronavirus disease 19|sars cov 2|2019 ncov|2019ncov|2019 n cov|2019n cov|ncov 2019|n cov 2019|coronavirus 2019|wuhan pneumonia|wuhan virus|wuhan coronavirus|coronavirus 2|covid-19|sars-cov-2|2019-ncov',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py365/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def checkCovid(text, covid_terms):\n",
    "    return bool(covid_terms.search(text))\n",
    "\n",
    "df_covid_new['is_covid'] = df_covid_new['body_text'].apply(checkCovid, covid_terms=covid_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid_only = df_covid_new[df_covid_new['is_covid']==True]\n",
    "len(df_covid_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5384, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid_only.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving Training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 use nltk to sentencize the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/xcs224u_student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "extra_abbreviations = ['ps',  'inc', 'Corp', 'Ltd', 'Co', 'pkt', 'Dz.Ap', 'Jr', 'jr', 'sp', 'Sp', 'poj',  'pseud', 'krypt', 'sygn', 'Dz.U', 'ws', 'itd', 'np', 'sanskryt', 'nr', 'gł', 'Takht', 'tzw', 't.zw', 'ewan', 'tyt', 'oryg', 't.j', 'vs', 'l.mn', 'l.poj' ]\n",
    "\n",
    "position_abbrev = ['Ks', 'Abp', 'abp','bp','dr', 'kard', 'mgr', 'prof', 'zwycz', 'hab', 'arch', 'arch.kraj', 'B.Sc', 'Ph.D', 'lek', 'med', 'n.med', 'bł', 'św', 'hr', 'dziek' ]\n",
    "\n",
    "quantity_abbrev = [ 'mln', 'obr./min','km/godz', 'godz', 'egz', 'ha', 'j.m', 'cal', 'obj', 'alk', 'wag' ] # not added: tys.\n",
    "\n",
    "actions_abbrev = ['tłum','tlum','zob','wym', 'pot', 'ww', 'ogł', 'wyd', 'min', 'm.i', 'm.in', 'in', 'im','muz','tj', 'dot', 'wsp', 'właść', 'właśc', 'przedr', 'czyt', 'proj', 'dosł', 'hist', 'daw', 'zwł', 'zaw' ]\n",
    "\n",
    "place_abbrev = ['Śl', 'płd', 'geogr']\n",
    "\n",
    "lang_abbrev = ['jęz','fr','franc', 'ukr', 'ang', 'gr', 'hebr', 'czes', 'pol', 'niem', 'arab', 'egip', 'hiszp', 'jap', 'chin', 'kor', 'tyb', 'wiet', 'sum']\n",
    "\n",
    "military_abbrev = ['kpt', 'kpr', 'obs', 'pil', 'mjr','płk', 'dypl', 'pp', 'gw', 'dyw', 'bryg', 'ppłk', 'mar', 'marsz', 'rez', 'ppor', 'DPanc', 'BPanc', 'DKaw', 'p.uł']\n",
    "\n",
    "extra_abbreviations= extra_abbreviations + position_abbrev + quantity_abbrev + place_abbrev + actions_abbrev + place_abbrev + lang_abbrev+military_abbrev\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "sent_tokenize = sentence_tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    with a 2000-year medicinal history, radix bupl...\n",
      "Name: body_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_covid_only['body_text'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_file(file_path):\\n    print(f\"Processing {file_path}\")\\n    lines = Path(file_path).read_text(\"utf-8\").split(\"\\n\")\\n    with ProcessPoolExecutor(10) as pool:\\n        return list(flatten(pool.map(process_line, lines)))\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(iterable):\n",
    "    return chain.from_iterable(iterable)\n",
    "\n",
    "def process_line(line):\n",
    "    try:\n",
    "        #doc = json.loads(line)\n",
    "        txt = re.sub(\"\\s+\", \" \", line)\n",
    "        sentences = [s for s in sent_tokenize(txt)]\n",
    "        windowed_sentences = []\n",
    "        for snt in range(len(sentences)):\n",
    "            windowed_sentences.append(\" \".join(sentences[snt: snt + 4]))\n",
    "        return windowed_sentences\n",
    "    except:\n",
    "        # print(f\"Could not parse line \\n{line}\\n\")\n",
    "        return []\n",
    "\n",
    "\"\"\"\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing {file_path}\")\n",
    "    lines = Path(file_path).read_text(\"utf-8\").split(\"\\n\")\n",
    "    with ProcessPoolExecutor(10) as pool:\n",
    "        return list(flatten(pool.map(process_line, lines)))\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRC_DATA_FPATH = \"pretrain/allpdf.train.sliding4-v1.txt\" #@param {type: \"string\"}\n",
    "with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    _ = list(flatten(df_covid_new[\"body_text\"].apply(process_line)))\n",
    "    for line in _:\n",
    "        fo.write(\"\\n\"+line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp = df_covid_only[\"body_text\"][:2].apply(process_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor line in list(flatten(temp)):\\n    print(line)\\n    print(\"+==================\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for line in list(flatten(temp)):\n",
    "    print(line)\n",
    "    print(\"+==================\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbuffer, BUFFER_SIZE = [], 100000\\nwith open(\"biorxiv_medrxiv.train.sliding4-v2.txt\", \"wt\") as file:\\n    for sentence in enumerate(flatten(process_file(f) for f in files)):\\n        if len(buffer) >= BUFFER_SIZE:\\n            file.write(\"\\n\".join(buffer))\\n            buffer.clear()\\n            print(i, end=\"\\r\")\\n        buffer.append(sentence)\\n    if len(buffer) > 0:\\n        file.write(\"\\n\".join(buffer))\\n        buffer.clear()\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use when processing txt documents \n",
    "\"\"\"\n",
    "buffer, BUFFER_SIZE = [], 100000\n",
    "with open(\"biorxiv_medrxiv.train.sliding4-v2.txt\", \"wt\") as file:\n",
    "    for sentence in enumerate(flatten(process_file(f) for f in files)):\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            file.write(\"\\n\".join(buffer))\n",
    "            buffer.clear()\n",
    "            print(i, end=\"\\r\")\n",
    "        buffer.append(sentence)\n",
    "    if len(buffer) > 0:\n",
    "        file.write(\"\\n\".join(buffer))\n",
    "        buffer.clear()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 722480\r\n",
      "drwxrwxr-x 3 xcs224u_student xcs224u_student      4096 May  1 07:55 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxrwxr-x 4 xcs224u_student xcs224u_student      4096 May  1 07:56 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-rw-r-- 1 xcs224u_student xcs224u_student       378 Apr 29 08:43 config.json\r\n",
      "-rw-rw-r-- 1 xcs224u_student xcs224u_student 696485376 May  1 07:55 covidonly.train.sliding4-v1.txt\r\n",
      "-rw-rw-r-- 1 xcs224u_student xcs224u_student  43303482 Apr 29 08:43 proc_dataset.txt\r\n",
      "-rw-rw-r-- 1 xcs224u_student xcs224u_student        16 Apr 29 08:43 tokenizer_config.json\r\n",
      "drwxrwxr-x 2 xcs224u_student xcs224u_student      4096 Apr 29 08:43 \u001b[01;34mtokenziner\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls -la pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= 'pretrain/covidonly.train.sliding4-v1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with a 2000-year medicinal history, radix bupleuri (chai hu in chinese) is believed to be one of th\n"
     ]
    }
   ],
   "source": [
    "f = open(file, \"r\")\n",
    "print(f.read(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lines[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py365",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
